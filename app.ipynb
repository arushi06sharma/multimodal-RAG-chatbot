{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and packages.\n",
    "import gradio as gr\n",
    "import os\n",
    "import getpass\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f88485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Google API Key an evironmental variable so that the API can be accessed.\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"GOOGLE_API_KEY\")\n",
    "# Creating the variables to access the LLM, embeddings and the respective vector store for the chatbot.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "# Declaring a State class.\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    history: List[dict]\n",
    "# Designing a system prompt to engineer the chatbot's responsivity to the users.\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly, helpful assistant with a conversational tone. \n",
    "\n",
    "Always prioritize the context provided when answering, and only use your own knowledge if the context is clearly unrelated.\n",
    "\n",
    "Your goal is to simplify complex ideas using:\n",
    "- Bullet points for clarity\n",
    "- Approachable analogies or metaphors\n",
    "- Occasional emojis (only where they help)\n",
    "\n",
    "Avoid jargon unless the user already used it. Match the user's tone and formality. Keep your answers informative but human.\n",
    "\n",
    "If the context doesn't mention the answer, feel free to respond from your own training, but *tell the user that's what you're doing*.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "User: What is the body effect in MOSFETs?\n",
    "\n",
    "Answer: Got you! Let's break it down:\n",
    "\n",
    "### The Body Effect ðŸ§²\n",
    "\n",
    "- It's a phenomenon in MOSFETs where the voltage between the source and body (VSB) affects how the transistor behaves\n",
    "- Even if you keep the gate voltage constant, changing VSB will change the threshold voltage (VT)\n",
    "- That means the transistor might turn on later or earlier\n",
    "\n",
    "Think of it like a tug-of-war between the gate and body â€” the body can pull the rope back a little, making it harder for the gate to win.\n",
    "\n",
    "Let me know if you want examples or more analogies! ðŸ˜Š\n",
    "\"\"\"\n",
    "# Formatting the system prompt along with the user input the model should be receiving.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "# Defining a retrieval function for the model to fetch an answer from the vector store.\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"]) \n",
    "    return {\"context\": retrieved_docs}\n",
    "# Defining a generation function which provides the model with the full history of the chat (to remember chat context better)\n",
    "# As well as invoking a response from the LLM, provided the context and the query.\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    # Collecting history of previous messages.\n",
    "    past_msgs = state.get(\"history\", [])\n",
    "    chat_history = []\n",
    "    # Storing the previous messages in a chat history variable for the model to have the context in easier grasp.\n",
    "    for msg in past_msgs:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            chat_history.append(HumanMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_history.append(AIMessage(content=msg[\"content\"]))\n",
    "        else:\n",
    "            print(\"\")\n",
    "    # Formatting messages for model understanding.\n",
    "    messages = prompt.format_messages(\n",
    "        question=state[\"question\"],\n",
    "        context=docs_content\n",
    "    )\n",
    "    # Updating history with the question/user query.\n",
    "    full_messages = chat_history + messages\n",
    "    response = llm.invoke(full_messages)\n",
    "    # Returning the LLM's answer.\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Creating a graph for the pipeline's execution.\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52801807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to extract images from the uploaded PDF, for the LLM to understand.\n",
    "def extract_images_from_pdf(pdf_path, output_folder=\"extracted_images\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    images = []\n",
    "    for page_number in range(len(doc)):\n",
    "        page = doc.load_page(page_number)\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"page{page_number+1}_img{img_index}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            with open(image_path, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            images.append((page_number+1, image_path))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fe1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to check whether the images extracted from the PDF are coherent, and not pixelated nonsensical pngs.\n",
    "def is_good_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return False\n",
    "    h, w = img.shape\n",
    "    if h < 100 or w < 100: # Making sure image dimension is proper.\n",
    "        return False\n",
    "    if np.var(img) < 500: # Low variance implies a flat image.\n",
    "        return False\n",
    "    edges = cv2.Canny(img, 100, 200) \n",
    "    if np.count_nonzero(edges) < 500: # Checking for edges, mostly used in diagrams and so.\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to caption the images extracted from the PDF to add to the vector store, enabling multi-modality.\n",
    "def caption_images(select_images):\n",
    "    captioner = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "    captions = {}\n",
    "    for image_file_path in select_images:\n",
    "        with open(image_file_path, \"rb\") as image_file:\n",
    "            encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        caption_query = HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": f\"An image is saved under: {image_file_path}. Start your answer with 'This is an image on page number --.' And continue. Remember the page number. Describe the local image.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{encoded_image}\"},\n",
    "            ]\n",
    "        )\n",
    "        try:\n",
    "            result_local = captioner.invoke([caption_query])\n",
    "            captions[image_file_path] = result_local.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error captioning {image_file_path}: {e}\")\n",
    "            captions[image_file_path] = \"Could not generate caption.\"\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55137df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to chunk/index the user's uploaded file, and prepare it for RAG. \n",
    "def handle_uploaded_file(tempfile):\n",
    "    # Loading the file uploaed by the user.\n",
    "    filepath = tempfile.name \n",
    "    try:\n",
    "        loader = PyPDFLoader(filepath)\n",
    "        docs = loader.load()\n",
    "    except Exception as e:\n",
    "        return f\"Couldn't parse the PDF: {e}\"\n",
    "    # Applying text splitting and storing them \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    # Appending the vector store with the indexed chunks.\n",
    "    _ = vector_store.add_documents(documents=all_splits)\n",
    "    # Extracting images from the uploaded file.\n",
    "    images = extract_images_from_pdf(filepath)\n",
    "    # Using OpenCV to assess the images and filter out irrelevant/corrupted ones.\n",
    "    good_images = []\n",
    "    for i in range(len(images)):\n",
    "        path = images[i][1]\n",
    "        if is_good_image(path):\n",
    "            good_images.append(path)\n",
    "        \n",
    "    # Captioning the images individually, storing them away in a list.\n",
    "    image_captions = caption_images(good_images)\n",
    "    # Converting the captions into a Langchain Document to add to the vector store.\n",
    "    image_docs = []\n",
    "    for image_path, caption in image_captions.items():\n",
    "        image_doc = Document(\n",
    "            page_content=caption,\n",
    "            metadata={\"source\": os.path.basename(image_path), \"type\": \"image\"}\n",
    "        )\n",
    "        image_docs.append(image_doc)\n",
    "    # Appending the image captions into the vector store.\n",
    "    _ = vector_store.add_documents(image_docs)\n",
    "    # Updating the status of the file indexing, chunking and extraction to the user.\n",
    "    return f\"File uploaded! {len(all_splits)} text chunks and {len(image_docs)} image captions indexed for RAG.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return the user's query, makes use of retrieve and generate as nodes.\n",
    "def handle_query(message, history):\n",
    "    response = graph.invoke({\"question\": message, \"history\": history})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading this chatbot based on a sample PDF: MOSFETs.pdf\n",
    "# Creating a replica of the chatbot exactly so that the main vector store remains clean.\n",
    "test_index = faiss.IndexFlatL2(embedding_dim)\n",
    "test_vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=test_index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "# Loading up basic necessities for our sample RAG pipeline for a given PDF.\n",
    "eval_loader = PyPDFLoader(\"MOSFETs.pdf\")\n",
    "eval_docs = eval_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "eval_splits = text_splitter.split_documents(eval_docs)\n",
    "_ = test_vector_store.add_documents(eval_splits)\n",
    "# Retrieval function which uses the vector store meant to store sample information from our static PDF.\n",
    "def retrieve_eval(state: State):\n",
    "    retrieved_docs = test_vector_store.similarity_search(state[\"question\"]) \n",
    "    return {\"context\": retrieved_docs}\n",
    "# Generate function remains the same.\n",
    "eval_graph_builder = StateGraph(State).add_sequence([retrieve_eval, generate])\n",
    "eval_graph_builder.add_edge(START, \"retrieve_eval\")\n",
    "eval_graph = eval_graph_builder.compile()\n",
    "# eval_graph will be the graph/pipeline we use for our evaluation purposes.\n",
    "# There is a file questions.csv that has a list of 10 questions that will be asked to the chatbot, and then used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the questions csv file and storing it into a dataframe to make grading easier.\n",
    "df = pd.read_csv(\"questions.csv\", encoding=\"windows-1252\")\n",
    "\n",
    "# Having the RAG pipeline run to answer test-queries. Useful for grading responses.\n",
    "questions = df[\"Questions\"].astype(str)\n",
    "true_responses = df[\"True Response\"].astype(str)\n",
    "llm_questions = []\n",
    "true_response = []\n",
    "\n",
    "for question in questions:\n",
    "    llm_questions.append(question)\n",
    "\n",
    "for response in true_responses:\n",
    "    true_response.append(response)\n",
    "\n",
    "llm_responses = []\n",
    "\n",
    "# Runs a loop for the LLM to answer all test queries stored in questions.csv\n",
    "for question in llm_questions:\n",
    "    response = eval_graph.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    llm_responses.append(response[\"answer\"])\n",
    "# Appending these responses to the dataframe\n",
    "df[\"LLM Response\"] = llm_responses\n",
    "# Saving the responses to a csv file\n",
    "df.to_csv('llm_output.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504882db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Gemini to grade the responses.\n",
    "llm_grader = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# System prompt for instructing the grader.\n",
    "system_prompt = \"\"\"\n",
    "    You are an expert response grader designed for LLMs, tasked with comparing the three given parameters:\n",
    "        The Query\n",
    "        The LLM's Response\n",
    "        The True Response\n",
    "    You are to compare the LLM's response to the True response and assign a grade to the LLM's response on a scale of 1-4,\n",
    "    where:\n",
    "        1 = The response is entirely irrelevant, and does not match the true response in words or gist.\n",
    "        2 = The response is similar to the truth, but lacks in either information or context.\n",
    "        3 = The response is close to the true response, but can be improved upon.\n",
    "        4 = The response matches the true response completely. It is perfect.\n",
    "    Only put out the grade (1-4) of the response after you have made the comparison. Think about it logically.\n",
    "    The first given string of a pair is the LLM's response, and the second string of the pair is the true response.\n",
    "\"\"\"\n",
    "# A template for the model to run on.\n",
    "query_answer = ChatPromptTemplate([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Compare:\\n\\n{user_input}\")\n",
    "])\n",
    "# Empty list for the answer pairs so it is easy to feed into the LLM for grading.\n",
    "answer_pair = []\n",
    "# Running a loop to store the responses pairwise.\n",
    "for i in range(len(llm_responses)):\n",
    "    pair = [llm_responses[i], true_response[i]]\n",
    "    answer_pair.append(pair)\n",
    "# Creating an empty list for the grades alloted by the grader.\n",
    "grades = []\n",
    "# Calling the LLM for every response-pair to be graded.\n",
    "for pairs in answer_pair:\n",
    "    formatted = query_answer.format_messages(user_input=pairs)\n",
    "    response = llm_grader.invoke(formatted)\n",
    "    grades.append(response.content)\n",
    "# Updating the dataframe with the new grades.\n",
    "df[\"Grade\"] = grades\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ed747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the statistics of the data of output.csv to assess model accuracy.\n",
    "import statistics\n",
    "grade_list = df[\"Grade\"].astype(int)\n",
    "avg = statistics.mean(grade_list)\n",
    "percentage = (avg/4) * 100\n",
    "percentage\n",
    "\n",
    "def update():\n",
    "    return f\"Current Stats of the Chatbot:\\n\\nAccuracy Score: {avg}/4\\nAccuracy Percentile: {percentage}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Gradio interface to handle the RAG pipeline. The frontend of the chatbot.\n",
    "with gr.Blocks(fill_width=True, fill_height=True) as demo:\n",
    "    gr.Markdown(\"# Conversational RAG \")\n",
    "    gr.Markdown(\"Upload a `.pdf` file, and ask the AI about it!\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            chatbox = gr.ChatInterface(\n",
    "                handle_query,\n",
    "                type=\"messages\",\n",
    "                flagging_mode=\"manual\",\n",
    "                flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
    "                save_history=True,\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            file_upload = gr.UploadButton(\"Upload PDF\", file_types=[\".pdf\"])\n",
    "            output = gr.Textbox()\n",
    "            file_upload.upload(fn=handle_uploaded_file, inputs=file_upload, outputs=output)\n",
    "            textbox = gr.Textbox(label=\"ðŸ“Š Chatbot Stats\", interactive=False)\n",
    "            refresh_button = gr.Button(\"Check Stats\")\n",
    "            refresh_button.click(fn=update, inputs=[], outputs=textbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching the demo.\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
